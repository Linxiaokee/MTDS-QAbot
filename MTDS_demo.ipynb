{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5408ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain.vectorstores import Chroma\n",
    "pro_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feebc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "openai_embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=pro_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4edbbb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from getFrom.getDatasFrom import getDatasFrom\n",
    "all_data = getDatasFrom(\"web\\static\\Data\\Chapter1\")\n",
    "\n",
    "datas = [d.get('data') for d in all_data]\n",
    "ids = [d.get('id') for d in all_data]\n",
    "metadatas = []\n",
    "source = [d.get('path') for d in all_data]\n",
    "chapters = [d.get('chapter') for d in all_data]\n",
    "\n",
    "for i, p in enumerate(source):\n",
    "    m_dict = dict(source=p, chapter=chapters[i])\n",
    "    metadatas.append(m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbe3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_texts(datas, openai_embedding, metadatas, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1e7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=pro_key,\n",
    "    temperature=0,\n",
    "    streaming=\"True\",\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(\n",
    "    openai_api_key=pro_key,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92d1b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationTokenBufferMemory \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\", \n",
    "    input_key='question', \n",
    "    output_key='answer', \n",
    "    max_token_limit=10,\n",
    "    return_messages=True\n",
    ")\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(\n",
    "    input_variables=['chat_history', 'question'], \n",
    "    output_parser=None, partial_variables={}, \n",
    "    template='''\n",
    "    Given the following conversation and a follow up question,\n",
    "    rephrase the follow up question to be a standalone question,\n",
    "    in Chinese.\\n\n",
    "    If you find the question seems irrelevant to the context, \n",
    "    however, don't try to rephrase it, output the original question instead.\n",
    "    \\n\\n \n",
    "    \n",
    "    Chat History:\\n{chat_history}\\n\n",
    "    Follow Up Input: {question}\\n\n",
    "    Standalone question:\n",
    "    ''', \n",
    "    template_format='f-string', \n",
    "    validate_template=True\n",
    ")\n",
    "NONSENSE_PROMPT = PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    output_parser=None, partial_variables={},\n",
    "    template='''\n",
    "    Please just repeat exactly the following sentence without any changes:\\n \n",
    "    {question}\n",
    "    ''',\n",
    "    template_format='f-string', \n",
    "    validate_template=True\n",
    ")\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=['context', 'question'], \n",
    "    output_parser=None, partial_variables={}, \n",
    "    template='''\n",
    "    Use the following knowledge triplets to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\n",
    "    {context}\\n\\n\n",
    "    Question: \n",
    "    {question}\\n\n",
    "    Helpful Answer:\", \n",
    "    ''',\n",
    "    template_format='f-string', \n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT, verbose=True)\n",
    "docs_chain = load_qa_chain(output_key='answer', llm=chat, chain_type='stuff', verbose=True)\n",
    "qa = ConversationalRetrievalChain(\n",
    "    memory=memory,\n",
    "    retriever=db.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=docs_chain,\n",
    "    return_generated_question=True,\n",
    "    verbose=True,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daa6793d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "<a name=\"bookmark3\"></a>**4、苹果设备安装 MTDS 需要信任 ，该怎么设置？**\n",
      "\n",
      "**解 决方案：**   打开 你 的 苹 果 设 备 ： 设置 - 通用 - 设备管理 - 信任此软件\n",
      "\n",
      "\n",
      "![](Aspose.Words.6e696103-a96d-42f3-be82-30adf0fec166.007.jpeg)\n",
      "\n",
      "\n",
      "\n",
      "<a name=\"bookmark9\"></a>**10、“我的欧派”手机端闪退？**\n",
      "\n",
      "**解决方案：**  重新扫码安装最新app  （二维码必须是最新的）\n",
      "\n",
      "注意：如重新下载后不能安装，请检查手机的版本是否最新，最好是该系统下\n",
      "\n",
      "最新的版本。（苹果手机ios系统是10以上， 安装系统是7以上可使用）\n",
      "\n",
      "\n",
      "\n",
      "**27、如何进入谷歌浏览器无痕模式？**\n",
      "\n",
      "**解决方案：**\n",
      "\n",
      "进入谷歌浏览器无痕模式方法：\n",
      "\n",
      "方式一：点开谷歌浏览器右上角三个点，点击【打开新的无痕式窗口】。\n",
      "\n",
      "方式二：快捷键 Ctrl+Shift+N ,直接打开新的无痕式窗口。\n",
      "\n",
      "进入无痕模式后，重新输入访问网址，此模式下浏览数据无缓存。\n",
      "\n",
      "\n",
      "![](Aspose.Words.6e696103-a96d-42f3-be82-30adf0fec166.056.jpeg)\n",
      "\n",
      "\n",
      "![](Aspose.Words.6e696103-a96d-42f3-be82-30adf0fec166.057.jpeg)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**MTDS0100027**\n",
      "\n",
      "\n",
      "<a name=\"bookmark16\"></a>**17、安装 APP 时提示“我的欧派”需要更新怎么办？**\n",
      "\n",
      "![](Aspose.Words.6e696103-a96d-42f3-be82-30adf0fec166.039.jpeg)\n",
      "\n",
      "**解决方案：** 检查系统版本是否为： ios15 。此测试版本，需待官方给出解决方案，\n",
      "\n",
      "故目前只能通过浏览器进入该网站进行操作：\n",
      "\n",
      "<https://ai.oppein.com/wxwork/#/>\n",
      "\n",
      "\n",
      "\n",
      "Human: 你确定吗\u001b[0m\n",
      "根据提供的信息，我可以确定以下内容：\n",
      "\n",
      "- 安装MTDS需要在苹果设备上进行设置信任。\n",
      "- 如果“我的欧派”手机端闪退，可以尝试重新扫码安装最新的app。\n",
      "- 进入谷歌浏览器无痕模式的方法有两种：点击谷歌浏览器右上角的三个点，选择“打开新的无痕式窗口”，或者使用快捷键Ctrl+Shift+N。\n",
      "- 如果安装APP时提示“我的欧派”需要更新，可以检查系统版本是否为ios15，并且目前只能通过浏览器进入该网站进行操作。\n",
      "\n",
      "如果你有其他问题，请提供更多详细信息。\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"你确定吗\"\n",
    "result = qa({'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "\n",
    "\n",
    "# Construct a ConversationalRetrievalChain with a streaming llm for combine docs\n",
    "# and a separate, non-streaming llm for question generation\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=pro_key)\n",
    "streaming_llm = ChatOpenAI(streaming=True, openai_api_key=pro_key, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "doc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=db.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59137ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_markdown_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    # 使用正则表达式提取图片链接\n",
    "    image_pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    image_sentences = re.findall(image_pattern, text)\n",
    "    return text,image_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, markdown_images = read_markdown_file('Data\\Chapter1\\Chapter1_3.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652676b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='Data\\Chapter1\\Chapter1_3.md'\n",
    "folder_path = file_name.rsplit('\\\\', 1)[0]\n",
    "folder_path = folder_path.replace('\\\\' ,'/')\n",
    "print(folder_path)\n",
    "images = []\n",
    "for i, image in enumerate(markdown_images):\n",
    "    image = '../' + folder_path + '/' + image\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
